{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gillette Tweet Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries to start your analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the gillette tweets for our analysis, use this encoding to avoid encoding errors\n",
    "#Save it to a dataframe \n",
    "\n",
    "df1 = pd.read_csv(\"gilettesent4.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barbasol Once Showed Gillette How To Make a Co...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BoycottGillette</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MeetBarbasol arbasol Once Showed Gillette How ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BoycottGillette MeetBarbasol destinyisbright C...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MeetBarbasol MeetBarbasol Barbasol Once Showed...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets     Label\n",
       "0  Barbasol Once Showed Gillette How To Make a Co...  Negative\n",
       "1                                    BoycottGillette  Negative\n",
       "2  MeetBarbasol arbasol Once Showed Gillette How ...  Negative\n",
       "3  BoycottGillette MeetBarbasol destinyisbright C...  Negative\n",
       "4  MeetBarbasol MeetBarbasol Barbasol Once Showed...  Negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Call head to see the first six records\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make two lists one with the tweets and one with the sentiment label\n",
    "\n",
    "Tweet = []\n",
    "Labels = []\n",
    "\n",
    "for row in df1[\"Tweets\"]: #Take the first column Tweets to clean it more\n",
    "    #tokenize words\n",
    "    words = word_tokenize(row) #Tokenize it or break it down into rows\n",
    "    #remove punctuations\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)] #Take out any string punctuation\n",
    "    english_stops = set(stopwords.words('english')) #remove stop words\n",
    "    characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"’\",\"“\",\"”\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\",\"'re\" ,\"'m\"] #remove other\n",
    "    #characters that may intefere\n",
    "    clean_words = [word for word in clean_words if word not in english_stops] #take out the stop words\n",
    "    clean_words = [word for word in clean_words if word not in set(characters_to_remove)] #take out characters for clean words\n",
    "    wordnet_lemmatizer = WordNetLemmatizer() #get the lemmas which breaks down the word but still keeps the semantic meaning\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words] #create a list of those lemmas\n",
    "    Tweet.append(lemma_list) #append them into the new Tweet list\n",
    "\n",
    "    for row in df1[\"Label\"]:\n",
    "        Labels.append(row) #Get a separate list for the label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined = zip(Tweet, Labels) #Zip both lists togther after the cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(words): #create a bag of words function \n",
    "    return dict([(word, True) for word in words]) #This will create a new dictionary with key value pairs of the tweet and label\n",
    "\n",
    "#The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). \n",
    "#In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding \n",
    "#grammar and even word order but keeping multiplicity. This will help with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Final_Data = [] #Create a new list\n",
    "\n",
    "for r, v in combined:\n",
    "    bag_of_words(r)\n",
    "    Final_Data.append((bag_of_words(r),v)) #make the dictionary keys from the tweets and sentiment and put it in new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "#Randomize the data that will be used for the model\n",
    "import random\n",
    "random.shuffle(Final_Data)\n",
    "print(len(Final_Data)) #This is how many records we have in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO on Naive Bayes PRE-MODEL\n",
    "\n",
    "The Naive Bayes algorithm is an intuitive method that uses the probabilities of each attribute belonging to each class to make a prediction. It is the supervised learning approach you would come up with if you wanted to model a predictive modeling problem probabilistically.\n",
    "\n",
    "Naive bayes simplifies the calculation of probabilities by assuming that the probability of each attribute belonging to a given class value is independent of all other attributes. This is a strong assumption but results in a fast and effective method.\n",
    "\n",
    "The probability of a class value given a value of an attribute is called the conditional probability. By multiplying the conditional probabilities together for each attribute for a given class value, we have a probability of a data instance belonging to that class.\n",
    "\n",
    "To make a prediction we can calculate probabilities of the instance belonging to each class and select the class value with the highest probability.\n",
    "\n",
    "Naive bases is often described using categorical data because it is easy to describe and calculate using ratios. A more useful version of the algorithm for our purposes supports numeric attributes and assumes the values of each numerical attribute are normally distributed (fall somewhere on a bell curve). Again, this is a strong assumption, but still gives robust results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance with Unigrams \n",
      "Accuracy: 0.7402597402597403\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = Final_Data[0:188], Final_Data[188:] #As a standard, split the data 30/70 of the 265 for the train and \n",
    "#test set. The train set is the portion of data that we are using to train our model, and the test set is where after the \n",
    "#the model has been trained, we will test its predictions on the test set and see how accurate it is in predicting what \n",
    "#was the sentiment of the test set from the sentiment given in the train set. Essentially, how accurate is our model in\n",
    "#predicting sentiment of tweets? \n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "from nltk import metrics\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) #This is our Naive Bayes classifier\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "print(\"Naive Bayes Performance with Unigrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO on Confusion Matrix (which is after this cell)\n",
    "\n",
    "Compute confusion matrix to evaluate the accuracy of a classification\n",
    "\n",
    "By definition a confusion matrix  is such that  is equal to the number of observations known to be in group  but predicted to be in group.\n",
    "\n",
    "Thus in binary classification, the count of true brand damaging negative (how many tweets that were negative the model predicted correctly), false brand damaging negatives - recall (how many tweets that were labeled as negative were positive), true brand positives (how many tweets that were positive were predicted correctly), and false brand positive positives - precision (how many tweets that were labeled as positive were actually negative). \n",
    "\n",
    "The F measure (F1 score or F score) is a measure of a test's accuracy and is defined as the weighted harmonic mean of the precision and recall of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnigramNB Results\n",
      "Brand Positive Precision: 0.5853658536585366\n",
      "Brand Positive Recall: 0.8888888888888888\n",
      "Brand Positive F-measure: 0.7058823529411764\n",
      "Brand Damaging Precision: 0.9166666666666666\n",
      "Brand Damaging Recall: 0.9166666666666666\n",
      "Brand Damaging F-measure: 0.7674418604651162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"UnigramNB Results\")\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testsets['Negative'], refsets['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    away = True           Positi : Negati =      5.2 : 1.0\n",
      "                    take = True           Positi : Negati =      4.4 : 1.0\n",
      "       thebestamancanget = True           Positi : Negati =      4.4 : 1.0\n",
      "                    real = True           Negati : Positi =      4.1 : 1.0\n",
      "                   itâs = True           Positi : Negati =      3.6 : 1.0\n",
      "                   point = True           Negati : Positi =      3.0 : 1.0\n",
      "                  people = True           Positi : Negati =      2.9 : 1.0\n",
      "                      'i = True           Positi : Negati =      2.8 : 1.0\n",
      "                     son = True           Positi : Negati =      2.8 : 1.0\n",
      "                 outrage = True           Positi : Negati =      2.8 : 1.0\n",
      "                      pa = True           Positi : Negati =      2.8 : 1.0\n",
      "                    look = True           Positi : Negati =      2.8 : 1.0\n",
      "                     day = True           Positi : Negati =      2.8 : 1.0\n",
      "                    iâm = True           Positi : Negati =      2.8 : 1.0\n",
      "                   video = True           Positi : Negati =      2.8 : 1.0\n",
      "                     fit = True           Positi : Negati =      2.8 : 1.0\n",
      "                  always = True           Positi : Negati =      2.8 : 1.0\n",
      "                      ad = True           Positi : Negati =      2.8 : 1.0\n",
      "                   thank = True           Positi : Negati =      2.7 : 1.0\n",
      "                    know = True           Negati : Positi =      2.5 : 1.0\n",
      "                    last = True           Negati : Positi =      2.5 : 1.0\n",
      "         gilletteboycott = True           Negati : Positi =      2.5 : 1.0\n",
      "              gillettead = True           Negati : Positi =      2.4 : 1.0\n",
      "             masculinity = True           Positi : Negati =      2.3 : 1.0\n",
      "                  better = True           Positi : Negati =      2.3 : 1.0\n",
      "                   think = True           Positi : Negati =      2.2 : 1.0\n",
      "                  asking = True           Positi : Negati =      2.2 : 1.0\n",
      "              commercial = True           Positi : Negati =      2.2 : 1.0\n",
      "              boycotting = True           Positi : Negati =      2.2 : 1.0\n",
      "                   viral = True           Positi : Negati =      2.0 : 1.0\n",
      "                    talk = True           Positi : Negati =      2.0 : 1.0\n",
      "                     way = True           Positi : Negati =      2.0 : 1.0\n",
      "                    body = True           Positi : Negati =      2.0 : 1.0\n",
      "                     mad = True           Positi : Negati =      2.0 : 1.0\n",
      "               beautiful = True           Positi : Negati =      2.0 : 1.0\n",
      "                   truly = True           Positi : Negati =      2.0 : 1.0\n",
      "                     see = True           Positi : Negati =      2.0 : 1.0\n",
      "                    dick = True           Positi : Negati =      2.0 : 1.0\n",
      "                    hear = True           Positi : Negati =      2.0 : 1.0\n",
      "                   metoo = True           Positi : Negati =      2.0 : 1.0\n",
      "                 replace = True           Positi : Negati =      2.0 : 1.0\n",
      "                   'this = True           Positi : Negati =      2.0 : 1.0\n",
      "                   might = True           Positi : Negati =      2.0 : 1.0\n",
      "                consumer = True           Positi : Negati =      2.0 : 1.0\n",
      "                  yâall = True           Positi : Negati =      2.0 : 1.0\n",
      "                  listen = True           Positi : Negati =      2.0 : 1.0\n",
      "                  reason = True           Positi : Negati =      2.0 : 1.0\n",
      "                    post = True           Positi : Negati =      2.0 : 1.0\n",
      "                   iâve = True           Positi : Negati =      2.0 : 1.0\n",
      "                    open = True           Positi : Negati =      2.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Top features from the model based on the tweets\n",
    "classifier.show_most_informative_features(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO on Decision Tree Algorithm PRE-MODEL\n",
    "\n",
    "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret\n",
    "\n",
    "The basic idea behind any decision tree algorithm is as follows:\n",
    "\n",
    "Select the best attribute using Attribute Selection Measures(ASM) to split the records.\n",
    "Make that attribute a decision node and breaks the dataset into smaller subsets.\n",
    "Starts tree building by repeating this process recursively for each child until one of the condition will match:\n",
    "All the tuples belong to the same attribute value.\n",
    "There are no more remaining attributes.\n",
    "There are no more instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnigramDT Results\n",
      "Accuracy: 0.6883116883116883\n",
      "Brand Positive Precision: 0.5853658536585366\n",
      "Brand Positive Recall: 0.8888888888888888\n",
      "Brand Positive F-measure: 0.7058823529411764\n",
      "Brand Damaging Precision: 0.9166666666666666\n",
      "Brand Damaging Recall: 0.7407407407407407\n",
      "Brand Damaging F-measure: 0.7674418604651162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set,  #making the necessary cutoffs to prune the tree\n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "refset = collections.defaultdict(set)\n",
    "testset = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = dt_classifier.classify(feats) #Start the model with the classifier\n",
    "    testset[observed].add(i)\n",
    "\n",
    "print(\"UnigramDT Results\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(dt_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testset['Negative'], refset['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO on Logistic Regression PRE-MODEL\n",
    "\n",
    "Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.\n",
    "\n",
    "Logistic Regression Assumptions\n",
    "Binary logistic regression requires the dependent variable to be binary.\n",
    "For a binary regression, the factor level 1 of the dependent variable should represent the desired outcome.\n",
    "Only the meaningful variables should be included.\n",
    "The independent variables should be independent of each other. That is, the model should have little or no multicollinearity.\n",
    "The independent variables are linearly related to the log odds.\n",
    "Logistic regression requires quite large sample sizes.\n",
    "Keeping the above assumptions in mind, let’s look at our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnigramsLogit Results\n",
      "Accuracy: 0.7142857142857143\n",
      "Brand Positive Precision: 0.5853658536585366\n",
      "Brand Positive Recall: 0.8888888888888888\n",
      "Brand Positive F-measure: 0.7058823529411764\n",
      "Brand Damaging Precision: 0.9166666666666666\n",
      "Brand Damaging Recall: 0.7540983606557377\n",
      "Brand Damaging F-measure: 0.7674418604651162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = logit_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"UnigramsLogit Results\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(logit_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testset['Negative'], refset['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))\n",
    "print(\"\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO on Support Vector Machine\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "Effective in high dimensional spaces.\n",
    "Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniigramSVM Recall\n",
      "Accuracy: 0.6493506493506493\n",
      "Brand Positive Precision: 0.5853658536585366\n",
      "Brand Positive Recall: 0.8888888888888888\n",
      "Brand Positive F-measure: 0.7058823529411764\n",
      "Brand Damaging Precision: 0.9166666666666666\n",
      "Brand Damaging Recall: 0.9166666666666666\n",
      "Brand Damaging F-measure: 0.7674418604651162\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = SVM_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"UniigramSVM Recall\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(SVM_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testsets['Negative'], refsets['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams!!\n",
    "\n",
    "A bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2\n",
    "\n",
    "Here is our sentence \"I read a book about the history of America.\"\n",
    " The machine wants to get the meaning of the sentence by separating it into small pieces. How should it do that? \n",
    "1. It can regard words one by one. This is unigram; each word is a gram.\n",
    " \"I\", \"read\", \"a\", \"book\", \"about\", \"the\", \"history\", \"of\", \"America\"\n",
    "2. It can regard words two at a time. This is bigram (digram); each two adjacent words create a bigram.\n",
    "\"I read\", \"read a\", \"a book\", \"book about\", \"about the\", \"the history\", \"history of\", \"of America\"\n",
    "3. It can regard words three at a time. This is trigram; each three adjacent words create a trigram.\n",
    "\"I read a\", \"read a book\", \"a book about\", \"book about the\", \"about the history\", \"the history of\", \"history of America\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined = zip(Tweet,Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)  \n",
    "    bigrams = bigram_finder.nbest(score_fn, n)  \n",
    "    return bag_of_words(bigrams) #Create the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Final_Data2 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_bigrams_words(z)\n",
    "    Final_Data2.append((bag_of_bigrams_words(z),e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "Naive Bayes Performance with Unigrams \n",
      "Accuracy: 0.7659574468085106\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(Final_Data2)\n",
    "print(len(Final_Data2))\n",
    "\n",
    "train_set, test_set = Final_Data2[0:218], Final_Data2[218:]\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "from nltk import metrics\n",
    "\n",
    "\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Performance with Unigrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "      ('gillette', 'ad') = True           Positi : Negati =      4.9 : 1.0\n",
      "('gillette', 'commercial') = True           Positi : Negati =      4.3 : 1.0\n",
      "('boycotting', 'gillette') = True           Positi : Negati =      3.8 : 1.0\n",
      "           ('ad', 'men') = True           Positi : Negati =      3.8 : 1.0\n",
      " ('threatened', 'razor') = True           Positi : Negati =      3.0 : 1.0\n",
      "         ('real', 'men') = True           Negati : Positi =      2.9 : 1.0\n",
      "     ('thing', 'offend') = True           Positi : Negati =      2.1 : 1.0\n",
      "    ('razor', 'company') = True           Positi : Negati =      2.1 : 1.0\n",
      "     ('buy', 'gillette') = True           Positi : Negati =      2.1 : 1.0\n",
      "('commercial', 'iâ\\x92ve') = True           Positi : Negati =      2.1 : 1.0\n",
      " ('razor', 'commerical') = True           Positi : Negati =      2.1 : 1.0\n",
      "       ('hint', 'razor') = True           Positi : Negati =      2.1 : 1.0\n",
      "   ('boy', 'gillettead') = True           Positi : Negati =      2.1 : 1.0\n",
      "    ('best', 'gillette') = True           Positi : Negati =      2.1 : 1.0\n",
      "        ('every', 'man') = True           Positi : Negati =      2.1 : 1.0\n",
      "           ('u', 'hint') = True           Positi : Negati =      2.1 : 1.0\n",
      "         ('offend', 'u') = True           Positi : Negati =      2.1 : 1.0\n",
      "('toxic', 'masculinity') = True           Positi : Negati =      1.9 : 1.0\n",
      "('gillette', 'gillettead') = True           Positi : Negati =      1.8 : 1.0\n",
      "    ('like', 'gillette') = True           Positi : Negati =      1.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramDT Results\n",
      "Brand Positive Precision: 0.7272727272727273\n",
      "Brand Positive Recall: 0.5\n",
      "Brand Positive F-measure: 0.5925925925925926\n",
      "Brand Damaging Precision: 0.7777777777777778\n",
      "Brand Damaging Recall: 0.7777777777777778\n",
      "Brand Damaging F-measure: 0.835820895522388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"BigramDT Results\")\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testsets['Negative'], refsets['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramDT Results\n",
      "Accuracy: 0.723404255319149\n",
      "Brand Positive Precision: 0.8\n",
      "Brand Positive Recall: 0.25\n",
      "Brand Positive F-measure: 0.38095238095238093\n",
      "Brand Damaging Precision: 0.7142857142857143\n",
      "Brand Damaging Recall: 0.7142857142857143\n",
      "Brand Damaging F-measure: 0.8219178082191781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = dt_classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print(\"BigramDT Results\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(dt_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testsets['Negative'], refsets['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramsLogit Results\n",
      "Accuracy: 0.6808510638297872\n",
      "Brand Positive Precision: 0.8\n",
      "Brand Positive Recall: 0.25\n",
      "Brand Positive F-measure: 0.38095238095238093\n",
      "Brand Damaging Precision: 0.7142857142857143\n",
      "Brand Damaging Recall: 0.7142857142857143\n",
      "Brand Damaging F-measure: 0.8219178082191781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = logit_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"BigramsLogit Results\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(logit_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testsets['Negative'], refsets['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))\n",
    "print(\"\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams Recall\n",
      "Accuracy: 0.6595744680851063\n",
      "Brand Positive Precision: 0.8\n",
      "Brand Positive Recall: 0.25\n",
      "Brand Positive F-measure: 0.38095238095238093\n",
      "Brand Damaging Precision: 0.7142857142857143\n",
      "Brand Damaging Recall: 0.7142857142857143\n",
      "Brand Damaging F-measure: 0.8219178082191781\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = SVM_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"Bigrams Recall\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(SVM_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testsets['Negative'], refsets['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams!!\n",
    "\n",
    "Trigrams are a special case of the n-gram, where n is 3. They are often used in natural language processing for performing statistical analysis of texts and in cryptography for control and use of ciphers and codes.\n",
    "\n",
    "Here is our sentence \"I read a book about the history of America.\"\n",
    " The machine wants to get the meaning of the sentence by separating it into small pieces. How should it do that? \n",
    "1. It can regard words one by one. This is unigram; each word is a gram.\n",
    " \"I\", \"read\", \"a\", \"book\", \"about\", \"the\", \"history\", \"of\", \"America\"\n",
    "2. It can regard words two at a time. This is bigram (digram); each two adjacent words create a bigram.\n",
    "\"I read\", \"read a\", \"a book\", \"book about\", \"about the\", \"the history\", \"history of\", \"of America\"\n",
    "3. It can regard words three at a time. This is trigram; each three adjacent words create a trigram.\n",
    "\"I read a\", \"read a book\", \"a book about\", \"book about the\", \"about the history\", \"the history of\", \"history of America\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined = zip(Tweet,Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "def bag_of_trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq, n=200):\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(words)  \n",
    "    trigrams = trigram_finder.nbest(score_fn, n)  \n",
    "    return bag_of_words(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "Naive Bayes Performance with Trigrams \n",
      "Accuracy: 0.7659574468085106\n"
     ]
    }
   ],
   "source": [
    "Final_Data3 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_trigrams_words(z)\n",
    "    Final_Data3.append((bag_of_trigrams_words(z),e))\n",
    "\n",
    "import random\n",
    "random.shuffle(Final_Data3)\n",
    "print(len(Final_Data3))\n",
    "\n",
    "train_set, test_set = Final_Data3[0:218], Final_Data3[218:]\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "from nltk import metrics\n",
    "\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Performance with Trigrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand Positive Precision: 1.0\n",
      "Brand Positive Recall: 0.3888888888888889\n",
      "Brand Positive F-measure: 0.56\n",
      "Brand Damaging Precision: 0.725\n",
      "Brand Damaging Recall: 0.6595744680851063\n",
      "Brand Damaging F-measure: 0.8405797101449275\n"
     ]
    }
   ],
   "source": [
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testset['Negative'], refset['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "('gillette', 'ad', 'men') = True           Positi : Negati =      3.9 : 1.0\n",
      "('hint', 'razor', 'commerical') = True           Positi : Negati =      1.3 : 1.0\n",
      " ('offend', 'u', 'hint') = True           Positi : Negati =      1.3 : 1.0\n",
      "('truly', 'appreciate', 'men') = True           Positi : Negati =      1.3 : 1.0\n",
      "('thing', 'offend', 'u') = True           Positi : Negati =      1.3 : 1.0\n",
      "  ('u', 'hint', 'razor') = True           Positi : Negati =      1.3 : 1.0\n",
      "('company', 'truly', 'appreciate') = True           Positi : Negati =      1.3 : 1.0\n",
      "('commercial', 'asking', 'better') = True           Negati : Positi =      1.3 : 1.0\n",
      "('new', 'shaver', 'need') = True           Negati : Positi =      1.3 : 1.0\n",
      "('shaver', 'need', 'new') = True           Negati : Positi =      1.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrigramDT Results\n",
      "Accuracy: 0.6170212765957447\n",
      "Brand Positive Precision: None\n",
      "Brand Positive Recall: 0.0\n",
      "Brand Positive F-measure: None\n",
      "Brand Damaging Precision: 0.6170212765957447\n",
      "Brand Damaging Recall: 0.6170212765957447\n",
      "Brand Damaging F-measure: 0.7631578947368421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = dt_classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print(\"TrigramDT Results\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(dt_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testsets['Negative'], refsets['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrigramsLogit Results\n",
      "Accuracy: 0.5106382978723404\n",
      "Brand Positive Precision: 0.4358974358974359\n",
      "Brand Positive Recall: 0.9444444444444444\n",
      "Brand Positive F-measure: 0.5964912280701755\n",
      "Brand Damaging Precision: 0.6170212765957447\n",
      "Brand Damaging Recall: 0.6170212765957447\n",
      "Brand Damaging F-measure: 0.7631578947368421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = logit_classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print(\"TrigramsLogit Results\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(logit_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testsets['Negative'], refsets['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams Results\n",
      "Accuracy: 0.6170212765957447\n",
      "Brand Positive Precision: 0.4358974358974359\n",
      "Brand Positive Recall: 0.9444444444444444\n",
      "Brand Positive F-measure: 0.5964912280701755\n",
      "Brand Damaging Precision: 0.6170212765957447\n",
      "Brand Damaging Recall: 0.6170212765957447\n",
      "Brand Damaging F-measure: 0.7631578947368421\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = SVM_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"Trigrams Results\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(SVM_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive Recall:', recall(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refsets['Positive'], testsets['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refsets['Negative'], testsets['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testsets['Negative'], refsets['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refsets['Negative'], testsets['Negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams!!! (combining all the grams!)\n",
    "\n",
    "N-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.\n",
    "\n",
    "When performing machine learning tasks related to natural language processing, we usually need to generate n-grams from input sentences. For example, in text classification tasks, in addition to using each individual token found in the corpus, we may want to add bi-grams or tri-grams as features to represent our documents. This post describes several different ways to generate n-grams quickly from input sentences in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined = zip(Tweet,Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq,\n",
    "n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return bigrams\n",
    "\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "\n",
    "# Import Bigram metrics - we will use these to identify the top 200 bigrams\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "def trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq,\n",
    "n=200):\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(words)\n",
    "    trigrams = trigram_finder.nbest(score_fn, n)\n",
    "    return trigrams\n",
    "\n",
    "\n",
    "def bag_of_Ngrams_words(words):\n",
    "    bigramBag = bigrams_words(words)\n",
    "    \n",
    "    #The following two for loops convert tuple into string\n",
    "    for b in range(0,len(bigramBag)):\n",
    "        bigramBag[b]=' '.join(bigramBag[b])\n",
    "   \n",
    "    trigramBag = trigrams_words(words)\n",
    "    for t in range(0,len(trigramBag)):\n",
    "        trigramBag[t]=' '.join(trigramBag[t])\n",
    "\n",
    "    return bag_of_words(trigramBag + bigramBag + words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Final_Data4 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_Ngrams_words(z)\n",
    "    Final_Data4.append((bag_of_Ngrams_words(z),e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "Naive Bayes Performance with Ngrams \n",
      "Accuracy: 0.6382978723404256\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(Final_Data4)\n",
    "print(len(Final_Data4))\n",
    "\n",
    "train_set, test_set = Final_Data4[0:218], Final_Data4[218:]\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "from nltk import metrics\n",
    "\n",
    "\n",
    "refset = collections. defaultdict(set)\n",
    "testset = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Performance with Ngrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             gillette ad = True           Positi : Negati =      7.0 : 1.0\n",
      "                    away = True           Positi : Negati =      5.7 : 1.0\n",
      "                    love = True           Positi : Negati =      4.5 : 1.0\n",
      "     gillette commercial = True           Positi : Negati =      4.5 : 1.0\n",
      "                    tell = True           Positi : Negati =      3.9 : 1.0\n",
      "                     son = True           Positi : Negati =      3.9 : 1.0\n",
      "                      pa = True           Positi : Negati =      3.9 : 1.0\n",
      "                    iâm = True           Positi : Negati =      3.9 : 1.0\n",
      "                    take = True           Positi : Negati =      3.9 : 1.0\n",
      "                     guy = True           Positi : Negati =      3.9 : 1.0\n",
      "                   right = True           Positi : Negati =      3.9 : 1.0\n",
      "         gilletteboycott = True           Negati : Positi =      3.8 : 1.0\n",
      "                    time = True           Positi : Negati =      3.6 : 1.0\n",
      "                 thought = True           Negati : Positi =      3.3 : 1.0\n",
      "                   truly = True           Positi : Negati =      3.1 : 1.0\n",
      "                 society = True           Positi : Negati =      3.1 : 1.0\n",
      "              threatened = True           Positi : Negati =      3.1 : 1.0\n",
      "                 outrage = True           Positi : Negati =      3.1 : 1.0\n",
      "                    look = True           Positi : Negati =      3.1 : 1.0\n",
      "                    dick = True           Positi : Negati =      3.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand Positive Precision: 0.52\n",
      "Brand Positive Recall: 0.7222222222222222\n",
      "Brand Positive F-measure: 0.6046511627906977\n",
      "Brand Damaging Precision: 0.7727272727272727\n",
      "Brand Damaging Recall: 0.7727272727272727\n",
      "Brand Damaging F-measure: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Brand Positive Precision:', precision(refset['Positive'], testset['Positive']))\n",
    "print('Brand Positive Recall:', recall(refset['Positive'], testset['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refset['Positive'], testset['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refset['Negative'], testset['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testset['Negative'], refset['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refset['Negative'], testset['Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NgramDT Results\n",
      "Accuracy: 0.6170212765957447\n",
      "Brand Positive Precision: 0.5\n",
      "Brand Positive Recall: 0.2777777777777778\n",
      "Brand Positive F-measure: 0.35714285714285715\n",
      "Brand Damaging Precision: 0.6486486486486487\n",
      "Brand Damaging Recall: 0.6486486486486487\n",
      "Brand Damaging F-measure: 0.7272727272727273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "refset = collections.defaultdict(set)\n",
    "testset = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = dt_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"NgramDT Results\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(dt_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refset['Positive'], testset['Positive']))\n",
    "print('Brand Positive Recall:', recall(refset['Positive'], testset['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refset['Positive'], testset['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refset['Negative'], testset['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testset['Negative'], refset['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refset['Negative'], testset['Negative']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NgramsLogit Recall\n",
      "Accuracy: 0.7021276595744681\n",
      "Brand Positive Precision: 0.5714285714285714\n",
      "Brand Positive Recall: 0.6666666666666666\n",
      "Brand Positive F-measure: 0.6153846153846154\n",
      "Brand Damaging Precision: 0.65\n",
      "Brand Damaging Recall: 0.65\n",
      "Brand Damaging F-measure: 0.7536231884057971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = logit_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"NgramsLogit Recall\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(logit_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refset['Positive'], testset['Positive']))\n",
    "print('Brand Positive Recall:', recall(refset['Positive'], testset['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refset['Positive'], testset['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refset['Negative'], testset['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testset['Negative'], refset['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refset['Negative'], testset['Negative']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NgramsSVM Recall\n",
      "Accuracy: 0.6170212765957447\n",
      "Brand Positive Precision: 0.5714285714285714\n",
      "Brand Positive Recall: 0.6666666666666666\n",
      "Brand Positive F-measure: 0.6153846153846154\n",
      "Brand Damaging Precision: 0.6170212765957447\n",
      "Brand Damaging Recall: 0.6170212765957447\n",
      "Brand Damaging F-measure: 0.7631578947368421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = SVM_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"NgramsSVM Recall\")\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(SVM_classifier, test_set))\n",
    "print('Brand Positive Precision:', precision(refset['Positive'], testset['Positive']))\n",
    "print('Brand Positive Recall:', recall(refset['Positive'], testset['Positive']))\n",
    "print('Brand Positive F-measure:', f_measure(refset['Positive'], testset['Positive']))\n",
    "print('Brand Damaging Precision:', precision(refset['Negative'], testset['Negative']))\n",
    "print('Brand Damaging Recall:', recall(testset['Negative'], refset['Negative']))\n",
    "print('Brand Damaging F-measure:', f_measure(refset['Negative'], testset['Negative']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
